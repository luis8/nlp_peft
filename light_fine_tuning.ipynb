{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Apply Lightweight Fine-Tuning to a Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Foundation Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "BASE_MODEL = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and compute functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_split(name: str, name_split: str):\n",
    "    dataset = datasets.load_dataset(name,  name_split, split='train').train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=23\n",
    "\n",
    "    )\n",
    "    return dataset['train'], dataset['test']\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "def predict(text: str, model, tokenizer):\n",
    "    input = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**input)\n",
    "\n",
    "    logits = output.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "    return predicted_class_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_dataset_split('financial_phrasebank', 'sentences_66agree')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], return_tensors=\"pt\", truncation=True, max_length=512, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label2id = {\"neutral\": 1, \"positive\": 2, \"negative\": 0}\n",
    "id2label = {1: \"neutral\", 2: \"positive\", 0: \"negative\"}\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for param in base_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 13/270 [01:31<30:05,  7.02s/it]\n",
      " 10%|█         | 27/270 [00:25<03:19,  1.22it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 10%|█         | 27/270 [00:30<03:19,  1.22it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6310212016105652, 'eval_accuracy': 0.7132701421800948, 'eval_runtime': 4.8194, 'eval_samples_per_second': 175.127, 'eval_steps_per_second': 1.452, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 54/270 [00:56<02:52,  1.25it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 20%|██        | 54/270 [01:01<02:52,  1.25it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6037357449531555, 'eval_accuracy': 0.7369668246445498, 'eval_runtime': 4.7471, 'eval_samples_per_second': 177.794, 'eval_steps_per_second': 1.475, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 81/270 [01:27<02:32,  1.24it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 30%|███       | 81/270 [01:32<02:32,  1.24it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.60226970911026, 'eval_accuracy': 0.735781990521327, 'eval_runtime': 4.787, 'eval_samples_per_second': 176.311, 'eval_steps_per_second': 1.462, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 108/270 [01:59<02:10,  1.24it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 40%|████      | 108/270 [02:04<02:10,  1.24it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5561473965644836, 'eval_accuracy': 0.759478672985782, 'eval_runtime': 4.9974, 'eval_samples_per_second': 168.887, 'eval_steps_per_second': 1.401, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 135/270 [02:33<01:49,  1.23it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 50%|█████     | 135/270 [02:38<01:49,  1.23it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5363507866859436, 'eval_accuracy': 0.7725118483412322, 'eval_runtime': 4.7383, 'eval_samples_per_second': 178.122, 'eval_steps_per_second': 1.477, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 162/270 [03:05<01:26,  1.25it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 60%|██████    | 162/270 [03:09<01:26,  1.25it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5357518792152405, 'eval_accuracy': 0.754739336492891, 'eval_runtime': 4.7566, 'eval_samples_per_second': 177.438, 'eval_steps_per_second': 1.472, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 189/270 [03:37<01:07,  1.20it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 70%|███████   | 189/270 [03:41<01:07,  1.20it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5431651473045349, 'eval_accuracy': 0.7725118483412322, 'eval_runtime': 4.8409, 'eval_samples_per_second': 174.348, 'eval_steps_per_second': 1.446, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 216/270 [04:10<00:46,  1.16it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 80%|████████  | 216/270 [04:15<00:46,  1.16it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5428694486618042, 'eval_accuracy': 0.7677725118483413, 'eval_runtime': 4.8347, 'eval_samples_per_second': 174.57, 'eval_steps_per_second': 1.448, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 243/270 [04:41<00:21,  1.25it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 90%|█████████ | 243/270 [04:46<00:21,  1.25it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5203144550323486, 'eval_accuracy': 0.7665876777251185, 'eval_runtime': 4.7641, 'eval_samples_per_second': 177.159, 'eval_steps_per_second': 1.469, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [05:13<00:00,  1.24it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "100%|██████████| 270/270 [05:18<00:00,  1.24it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5291668772697449, 'eval_accuracy': 0.7760663507109005, 'eval_runtime': 4.81, 'eval_samples_per_second': 175.466, 'eval_steps_per_second': 1.455, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 270/270 [05:19<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 319.7794, 'train_samples_per_second': 105.479, 'train_steps_per_second': 0.844, 'train_loss': 0.5793722647207754, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5203144550323486, 'eval_accuracy': 0.7665876777251185, 'eval_runtime': 4.567, 'eval_samples_per_second': 184.804, 'eval_steps_per_second': 1.533, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Train Base Model\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# We need to remove the sentence column from the datasets. Because there is a bug in the current version of the library\n",
    "train_base = train.map(tokenize, batched=True).remove_columns([\"sentence\"])\n",
    "test_base = test.map(tokenize, batched=True).remove_columns([\"sentence\"])\n",
    "\n",
    "# Rename the label column to labels because the trainer expects that name\n",
    "train_base = train_base.rename_column(\"label\", \"labels\")\n",
    "test_base = test_base.rename_column(\"label\", \"labels\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/financial_phrasebank\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-3,\n",
    "\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_base,\n",
    "    eval_dataset=test_base,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    ")\n",
    "trainer.train()\n",
    "base_evaluation = trainer.evaluate()\n",
    "print(base_evaluation)\n",
    "base_model.save_pretrained(\"gpt-lightfinetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt-lightfinetuned\")\n",
    "\n",
    "test_df = pd.DataFrame(test)\n",
    "test_small = test_df.sample(100)\n",
    "test_small['predicted'] = test_small['sentence'].apply(lambda x: predict(x, base_model, tokenizer))\n",
    "test_small['correct'] = test_small['predicted'] == test_small['label']\n",
    "prediction_percentage = test_small['correct'].mean()\n",
    "print(f\"Accuracy: {prediction_percentage:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Lightweight Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.12/site-packages/peft/tuners/lora/layer.py:711: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "config = LoraConfig()\n",
    "number_labels = len(list(id2label.keys()))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=number_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "lora_model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                \n",
      "  3%|▎         | 9/270 [01:26<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6682124733924866, 'eval_accuracy': 0.7298578199052133, 'eval_runtime': 5.9436, 'eval_samples_per_second': 142.003, 'eval_steps_per_second': 2.355, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [02:30<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40280744433403015, 'eval_accuracy': 0.8447867298578199, 'eval_runtime': 5.3442, 'eval_samples_per_second': 157.929, 'eval_steps_per_second': 2.62, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [03:34<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37071385979652405, 'eval_accuracy': 0.8518957345971564, 'eval_runtime': 5.3252, 'eval_samples_per_second': 158.493, 'eval_steps_per_second': 2.629, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [04:37<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3019632399082184, 'eval_accuracy': 0.8838862559241706, 'eval_runtime': 5.3393, 'eval_samples_per_second': 158.073, 'eval_steps_per_second': 2.622, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [05:40<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.284106969833374, 'eval_accuracy': 0.9016587677725119, 'eval_runtime': 5.2995, 'eval_samples_per_second': 159.26, 'eval_steps_per_second': 2.642, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [06:44<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2787133753299713, 'eval_accuracy': 0.8992890995260664, 'eval_runtime': 5.6155, 'eval_samples_per_second': 150.298, 'eval_steps_per_second': 2.493, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [07:47<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33058205246925354, 'eval_accuracy': 0.8981042654028436, 'eval_runtime': 5.3342, 'eval_samples_per_second': 158.224, 'eval_steps_per_second': 2.625, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [08:50<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3149661421775818, 'eval_accuracy': 0.8945497630331753, 'eval_runtime': 5.312, 'eval_samples_per_second': 158.887, 'eval_steps_per_second': 2.636, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [09:53<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3133288323879242, 'eval_accuracy': 0.9016587677725119, 'eval_runtime': 5.2807, 'eval_samples_per_second': 159.829, 'eval_steps_per_second': 2.651, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 9/270 [10:19<08:39,  1.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3113, 'grad_norm': 0.6535989046096802, 'learning_rate': 0.00011320754716981132, 'epoch': 9.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                 \n",
      "  3%|▎         | 9/270 [10:57<08:39,  1.99s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3258794844150543, 'eval_accuracy': 0.8992890995260664, 'eval_runtime': 5.6941, 'eval_samples_per_second': 148.225, 'eval_steps_per_second': 2.459, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 530/530 [10:35<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 635.1817, 'train_samples_per_second': 53.103, 'train_steps_per_second': 0.834, 'train_loss': 0.3001336776985312, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:04<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# We need to remove the sentence column from the datasets. Because there is a bug in the current version of the library\n",
    "train_peft = train.map(tokenize, batched=True).remove_columns([\"sentence\"])\n",
    "test_peft = test.map(tokenize, batched=True).remove_columns([\"sentence\"])\n",
    "\n",
    "# Rename the label column to labels because the trainer expects that name\n",
    "train_peft = train_peft.rename_column(\"label\", \"labels\")\n",
    "test_peft = test_peft.rename_column(\"label\", \"labels\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/financial_phrasebank_peft\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-3,\n",
    "\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_peft,\n",
    "    eval_dataset=test_peft,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    ")\n",
    "trainer.train()\n",
    "peft_evaluation = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained('gpt-lora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference Using the Fine-Tuned Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "lora_model_finetuned = AutoPeftModelForCausalLM.from_pretrained('gpt-lora')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "test_df_finetuned = pd.DataFrame(test)\n",
    "test_small_finetuned = test_df_finetuned.sample(100)\n",
    "test_small_finetuned['predicted'] = test_small_finetuned['sentence'].apply(lambda x: predict(x, lora_model_finetuned, tokenizer))\n",
    "test_small_finetuned['correct'] = test_small_finetuned['predicted'] == test_small_finetuned['label']\n",
    "prediction_percentage_finetuned_peft = test_small_finetuned['correct'].mean()\n",
    "print(f\"Accuracy: {prediction_percentage_finetuned_peft:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: {'eval_loss': 0.5203144550323486, 'eval_accuracy': 0.7665876777251185, 'eval_runtime': 4.567, 'eval_samples_per_second': 184.804, 'eval_steps_per_second': 1.533, 'epoch': 10.0}\n",
      "Peft model: {'eval_loss': 0.2787133753299713, 'eval_accuracy': 0.8992890995260664, 'eval_runtime': 5.1844, 'eval_samples_per_second': 162.796, 'eval_steps_per_second': 2.7, 'epoch': 10.0}\n",
      "Accuracy base model: 0.75\n",
      "Accuracy peft model: 0.72 Improvement: -0.03\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base model: {base_evaluation}\")\n",
    "print(f\"Peft model: {peft_evaluation}\")\n",
    "\n",
    "# Random 100 samples\n",
    "print(f\"Accuracy base model: {prediction_percentage:.2f}\")\n",
    "print(f\"Accuracy peft model: {prediction_percentage_finetuned_peft:.2f}\", f\"Improvement: {prediction_percentage_finetuned_peft - prediction_percentage:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
